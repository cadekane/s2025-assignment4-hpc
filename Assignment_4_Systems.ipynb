{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa904119-23ec-4321-98e0-b47880337e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs336_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba1b2f2-5281-48e3-9024-784d04adbac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2456018/1114479936.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/cadekane/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# Original benchmarking script –> got up to large backward with 32GB GPU\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import timeit\n",
    "import numpy as np\n",
    "from cs336_basics.model import BasicsTransformerLM\n",
    "\n",
    "# Define model sizes\n",
    "MODEL_SIZES = {\n",
    "    \"small\":   {\"d_model\": 768,  \"d_ff\": 3072,  \"num_layers\": 12, \"num_heads\": 12},\n",
    "    \"medium\":  {\"d_model\": 1024, \"d_ff\": 4096,  \"num_layers\": 24, \"num_heads\": 16},\n",
    "    \"large\":   {\"d_model\": 1280, \"d_ff\": 5120,  \"num_layers\": 36, \"num_heads\": 20},\n",
    "    \"xl\":      {\"d_model\": 1600, \"d_ff\": 6400,  \"num_layers\": 48, \"num_heads\": 25},\n",
    "    \"2.7B\":    {\"d_model\": 2560, \"d_ff\": 10240, \"num_layers\": 32, \"num_heads\": 32},\n",
    "}\n",
    "\n",
    "def benchmark(model_size=\"small\", batch_size=8, seq_len=512, steps=5, warmup=1, backward=False):\n",
    "    config = MODEL_SIZES[model_size]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=50257,\n",
    "        context_length=seq_len,\n",
    "        d_model=config[\"d_model\"],\n",
    "        d_ff=config[\"d_ff\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        num_heads=config[\"num_heads\"],\n",
    "        attn_pdrop=0.1,\n",
    "        residual_pdrop=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    dummy_input = torch.randint(0, 50257, (batch_size, seq_len), device=device)\n",
    "    dummy_target = torch.randint(0, 50257, (batch_size, seq_len), device=device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    times = []\n",
    "\n",
    "    # Warm-up steps\n",
    "    for _ in range(warmup):\n",
    "        output = model(dummy_input)\n",
    "        if backward:\n",
    "            loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), dummy_target.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # Timed steps\n",
    "    for _ in range(steps):\n",
    "        start = timeit.default_timer()\n",
    "        output = model(dummy_input)\n",
    "        if backward:\n",
    "            loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), dummy_target.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        end = timeit.default_timer()\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_time = np.mean(times)\n",
    "    std_dev = np.std(times)\n",
    "    kind = \"forward+backward\" if backward else \"forward\"\n",
    "    print(f\"[{model_size}] {kind} pass — Avg: {avg_time:.4f}s, Std Dev: {std_dev:.4f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_size\", type=str, choices=MODEL_SIZES.keys(), default=\"small\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=512)\n",
    "    parser.add_argument(\"--steps\", type=int, default=5)\n",
    "    parser.add_argument(\"--warmup\", type=int, default=1)\n",
    "    parser.add_argument(\"--backward\", action=\"store_true\", help=\"Include backward pass\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    benchmark(\n",
    "        model_size=args.model_size,\n",
    "        batch_size=args.batch_size,\n",
    "        seq_len=args.seq_len,\n",
    "        steps=args.steps,\n",
    "        warmup=args.warmup,\n",
    "        backward=args.backward\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54684e-1165-4d26-b625-4ced71bf9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# benchmark(\"small\", backward=False)\n",
    "# benchmark(\"small\", backward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e787d0f-662f-4fc6-b279-b3507d7698a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46109c8c-a863-43cc-9a47-d52bf23ed375",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.56 GiB of which 5.44 MiB is free. Including non-PyTorch memory, this process has 15.55 GiB memory in use. Of the allocated memory 15.40 GiB is allocated by PyTorch, and 6.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Forward-only benchmarks\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Forward + Backward benchmarks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(model_size, batch_size, seq_len, steps, warmup, backward)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Warm-up steps\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(warmup):\n\u001b[0;32m---> 38\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backward:\n\u001b[1;32m     40\u001b[0m         loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), dummy_target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ece491/s2025-assignment4-hpc/cs336-basics/cs336_basics/model.py:173\u001b[0m, in \u001b[0;36mBasicsTransformerLM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    170\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_pdrop)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# (batch size, sequence_length, d_model)\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# (batch size, sequence_length, d_model)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\n",
      "File \u001b[0;32m~/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ece491/s2025-assignment4-hpc/cs336-basics/cs336_basics/model.py:314\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    x: FloatTensor of shape `(batch_size, sequence_length, d_model)`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    FloatTensor of shape `(batch_size, sequence_length, d_model)`.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# NOTE: this is a pre-norm Transformer, and differs from the original\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# description in the paper.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# Apply the multi-head self-attention sublayer\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m x_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_pdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     x_attn \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x_attn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_pdrop)\n",
      "File \u001b[0;32m~/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs336_systems/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ece491/s2025-assignment4-hpc/cs336-basics/cs336_basics/model.py:479\u001b[0m, in \u001b[0;36mCausalMultiHeadSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    475\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(\n\u001b[1;32m    476\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones(sequence_length, sequence_length, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    477\u001b[0m )\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Shape: (batch_size, num_heads, sequence_length, d_k)\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_pdrop\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# Now, we need to \"concat\" the outputs of the different heads by reshaping to\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# (batch_size, sequence_length, num_heads * d_v).\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# First, we need to undo the earlier transpose to go from (batch_size, num_heads, sequence_length, d_v)\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# to (batch_size, sequence_length, num_heads, d_v)\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# Then, we combine the last two dimensions via reshaping to (batch_size, sequence_length, num_heads * d_v)\u001b[39;00m\n\u001b[1;32m    487\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    488\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;241m.\u001b[39mview(batch_size, sequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_v \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m    491\u001b[0m )\n",
      "File \u001b[0;32m~/ece491/s2025-assignment4-hpc/cs336-basics/cs336_basics/model.py:382\u001b[0m, in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(K, Q, V, mask, pdrop)\u001b[0m\n\u001b[1;32m    380\u001b[0m d_k \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Shape: (batch_size, sequence_length, sequence_length)\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m (\u001b[43mQ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_k)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Apply the mask, if we have one.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.56 GiB of which 5.44 MiB is free. Including non-PyTorch memory, this process has 15.55 GiB memory in use. Of the allocated memory 15.40 GiB is allocated by PyTorch, and 6.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Forward-only benchmarks\n",
    "benchmark(\"medium\", backward=False)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Forward + Backward benchmarks\n",
    "benchmark(\"medium\", backward=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a6b45c-3fd2-4422-8279-9c45ab36e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\"large\", backward=False)\n",
    "\n",
    "benchmark(\"large\", backward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe613d2-0686-43cc-ab5d-4534ebf1f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\"xl\", backward=False)\n",
    "\n",
    "benchmark(\"xl\", backward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59d0bc-9d19-4789-90c2-cf5636f59f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(\"2.7B\", backward=False)\n",
    "\n",
    "benchmark(\"2.7B\", backward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6cf41e4-a7d3-4016-a311-f2821c0f6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 30 09:36:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 5000                On  |   00000000:89:00.0 Off |                  Off |\n",
      "| 33%   33C    P8             10W /  230W |   15920MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         2453910      C   ...envs/cs336_systems/bin/python      15916MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d8850-02ba-4eb6-a05a-2b35e82db9f2",
   "metadata": {},
   "source": [
    "## Pytorch profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2d7da-a234-4223-9ef7-be381d1c90d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2992203-338b-4d5f-9025-816f07d64a5c",
   "metadata": {},
   "source": [
    "# 3 Distributed data parallel training\n",
    "\n",
    "## 3.1 SINGLE node distributed communication in Pytorch\n",
    "best practices for benchmarking distributed applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75040add-3d99-43e1-ab46-38e64441394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def run(rank, world_size, backend, device_type, tensor_size_MB, return_dict):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
    "\n",
    "    dtype = torch.float32\n",
    "    numel = int((tensor_size_MB * 1024 * 1024) / 4)\n",
    "    device = torch.device(f\"{device_type}:{rank % torch.cuda.device_count()}\" if device_type == \"cuda\" else \"cpu\")\n",
    "    tensor = torch.ones(numel, dtype=dtype, device=device)\n",
    "\n",
    "    # Warm-up\n",
    "    for _ in range(5):\n",
    "        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "        if device_type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    # Timed\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    dist.destroy_process_group()\n",
    "    return_dict[rank] = end - start\n",
    "\n",
    "def benchmark(backend, device_type, processes, sizes_MB):\n",
    "    results = []\n",
    "    for size_MB in sizes_MB:\n",
    "        manager = mp.Manager()\n",
    "        return_dict = manager.dict()\n",
    "        mp.spawn(run,\n",
    "                 args=(processes, backend, device_type, size_MB, return_dict),\n",
    "                 nprocs=processes,\n",
    "                 join=True)\n",
    "        times = list(return_dict.values())\n",
    "        avg_time = sum(times) / len(times)\n",
    "        results.append((backend, device_type, processes, size_MB, avg_time))\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sizes_MB = [0.5, 1, 10, 50, 100, 500, 1024]\n",
    "    configs = [\n",
    "        (\"gloo\", \"cpu\"),\n",
    "        (\"gloo\", \"cuda\"),\n",
    "        (\"nccl\", \"cuda\"),\n",
    "    ]\n",
    "    all_results = []\n",
    "\n",
    "    for backend, device in configs:\n",
    "        for proc in [2, 4, 6]:\n",
    "            if device == \"cuda\" and not torch.cuda.is_available():\n",
    "                continue\n",
    "            if device == \"cuda\" and proc > torch.cuda.device_count():\n",
    "                continue\n",
    "            print(f\"Running: {backend} on {device} with {proc} processes\")\n",
    "            results = benchmark(backend, device, proc, sizes_MB)\n",
    "            all_results.extend(results)\n",
    "\n",
    "    # Save or print results\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(all_results, columns=[\"Backend\", \"Device\", \"Processes\", \"Size_MB\", \"Time_s\"])\n",
    "    print(df)\n",
    "    df.to_csv(\"allreduce_benchmark_results.csv\", index=False)\n",
    "\n",
    "    # Plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for (backend, device), group in df.groupby([\"Backend\", \"Device\"]):\n",
    "        for nproc in sorted(group[\"Processes\"].unique()):\n",
    "            sub = group[group[\"Processes\"] == nproc]\n",
    "            plt.plot(sub[\"Size_MB\"], sub[\"Time_s\"], label=f\"{backend}+{device} ({nproc} proc)\", marker=\"o\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Tensor Size (MB)\")\n",
    "    plt.ylabel(\"AllReduce Time (s)\")\n",
    "    plt.title(\"AllReduce Performance\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"allreduce_benchmark_plot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179fa02-d8e5-4f68-a368-9973908b2f88",
   "metadata": {},
   "source": [
    "## 3.3 Naive implementation of distributed data parallel (DDP) training\n",
    "Each device initially constructs a randomly initialized model, then we use broadcast collective communication, so each device holds an identical copy of the parameters and optimizer states for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520141a6-f55e-4570-bb66-e2549a8cffab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_systems",
   "language": "python",
   "name": "cs336_systems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
